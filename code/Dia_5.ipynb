{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67fa111b-d392-4779-a7ed-64a2fd0c2cd7",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ulises1229/Intro-ML-Python/blob/master/code/Dia_5.ipynb\\\" target=\"_parent\\\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\"Open In Colab\\\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10af191-9600-4fbb-9b74-888705cc2b06",
   "metadata": {},
   "source": [
    "# Día 5: Aprendizaje profundo (Deep Learning)\n",
    "<ul>\n",
    "    <li><strong>Autor:</strong> Walter Rosales</li>\n",
    "    <li><strong>Contacto:</strong> <a href=\"mailto:walt22r@outlook.com\">walt22r@outlook.com</a>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600233bc-d20c-41c5-b7ff-828ac09633a5",
   "metadata": {},
   "source": [
    "---\n",
    "#### Deep Learning, Machine Learning e Inteligencia Artificial ¿son lo mismo?\n",
    "\n",
    "> <img src=\"../figs/nvidia-ai-comparison.png\" width=700/>\n",
    ">\n",
    "> Fig 1. Linea temporal del surgimiento de Inteligencia Artificial, Machine Learning y Deep Learning. \n",
    "> \n",
    "> Fuente: <a href=\"https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/\">Blog de NVIDIA</a>\n",
    "\n",
    "Como se puede apreciar en la Fig. 1, en realidad una es un subconjunto de la otra. En particular Deep Learning es un subconjunto de Machine Learning, que a su vez lo es de algo mucho más grande que se denomina Inteligencia Artificial. Puedes encontrar más información también en [este artículo de IBM](https://www.ibm.com/cloud/blog/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks).\n",
    "\n",
    "Existe una particularidad que define a los algoritmos de Deep Learning dentro de los de Machine Learning, y es su capacidad de aprender sin necesidad de una intervención/decisión humana directo respecto al modelo (ver Fig. 2).\n",
    "\n",
    "> <img src=\"../figs/ML_vs_DL.png\" width=700/>\n",
    ">\n",
    "> Fig. 2. Comparación entre algoritmos de Machine Learning y Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a8e9ab-aac0-40d1-bc2e-a7c8058ad6cd",
   "metadata": {},
   "source": [
    "---\n",
    "## Deep Learning\n",
    "\n",
    "Conjunto de algoritmos que utilizan una estrategia de capas ocultas (_hidden layers_) con funciones de activación no lineales internas entre una capa de entrada y una de salida. Es decir, modelos bioinspirados por el funcionamiento general del cerebro biológico, como se establece de forma general en [este otro artículo de IBM](https://www.ibm.com/cloud/learn/deep-learning).\n",
    "\n",
    "> <img src=\"../figs/bioinspired_DL_neuron.png\" width=440>\n",
    "> \n",
    "> Fig. 3: a) Esquema de una neurona del sistema nervioso biológico. b) Representación matemática/computacional de una neurona artificial.\n",
    "> \n",
    "> Fuente: [Zhu, G., Jiang, B., Tong, L., Xie, Y., Zaharchuk, G., &amp; Wintermark, M. (2019). Applications of deep learning to neuro-imaging techniques. Frontiers in Neurology, 10. https://doi.org/10.3389/fneur.2019.00869 ](https://www.frontiersin.org/articles/10.3389/fneur.2019.00869/full)\n",
    "\n",
    "En la Fig. 3 podemos observar cómo se plantea la similitud entre una neurona biolígica, y se respectivo modelo matemático que se utiliza para emular su respuesta ante estímulos y que sirve como base para construir redes neuronales artificiales como es mostrado en la Fig. 4.\n",
    "\n",
    "> <img src=\"../figs/bioinspired_DL_network.jpg\" width=600>\n",
    "> \n",
    "> Fig. 4: Emulación de una red neuronal artificial (B) y su representación análoga en el sistema nervioso biológico (A) para clasificar a partir de datos gráficos (imagen) como entrada.\n",
    ">\n",
    "> Fuente: [Mohamed K.S. (2020) Deep Learning and Cognitive Computing: Pillars and Ladders. In: Neuromorphic Computing and Beyond. Springer, Cham. https://doi.org/10.1007/978-3-030-37224-8_4](https://link.springer.com/chapter/10.1007/978-3-030-37224-8_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168a113-4055-4b88-af01-b8a633980cf2",
   "metadata": {},
   "source": [
    "### Modelos de Deep Learning\n",
    "\n",
    "Existen diversos modelos de Deep Learning, cada día habiendo más y más, siendo los más comúnes los feed-forward neural network (también conocidos como ANN de Artificial Neural Network), Convolutional Neural Networks (CNN), Recurrent Neural Network (RNN) y muchas más (ver Fig. 5). Y sus actuales aplicaciones son el mejor referente de su éxito, algunos de los más importantes están mencionados en [Wikipedia](https://en.wikipedia.org/wiki/Deep_learning#Applications).\n",
    "\n",
    "> <img src=\"../figs/DL_architectures.png\" width=500 />\n",
    "> \n",
    "> Fig. 5: Diagramas de las diferentes arquitecturas de redes neuronales más comúnes en la literatura actual. \n",
    ">\n",
    "> Fuente: [Kwon, S.H.; Kim, J.H. (2021) Machine Learning and Urban Drainage Systems: State-of-the-Art Review. Water, 13. https://doi.org/10.3390/w13243545](https://www.mdpi.com/2073-4441/13/24/3545)\n",
    "\n",
    "Siguiendo todos estos un patrón esencial, cuentan con tres componentes básicos: una capa de entrada, una o varias capas ocultas y una capa de salida (ver Fig. 6), siendo la primera donde ingresan los datos, las siguientes donde se procesa y se _aprende_ y la última aquella que da la _predicción_ del modelo.\n",
    "\n",
    "> <img src=\"../figs/ANN_architecture.png\" width=600 />\n",
    ">\n",
    "> Fig. 6: Esquema general de un modelo de red neuronal artificial (ANN) ejemplificando los tres tipos de capas que la componen: entrada ($i$), ocultas ($h_n$) y salida ($o$).\n",
    ">\n",
    "> Fuente: [Bre, F., Gimenez, J. M., &amp; Fachinotti, V. D. (2018). Prediction of wind pressure coefficients on building surfaces using artificial neural networks. Energy and Buildings, 158, 1429–1441. https://doi.org/10.1016/j.enbuild.2017.11.045](https://www.sciencedirect.com/science/article/abs/pii/S0378778817325501?via%3Dihub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2d7a7a-2a07-4e6c-b0ee-9525c38159a7",
   "metadata": {},
   "source": [
    "### Requerimientos para crear un modelo de DL\n",
    "\n",
    "Para crear un modelo de DL es necesario contar con los siguientes cuatro pilares básicos:\n",
    "1. Datos\n",
    "2. Arquitectura\n",
    "3. Función de pérdida\n",
    "4. Optimizador\n",
    "\n",
    "#### Datos\n",
    "Al ser los algoritmos de DL pertenecientes a la categoría de Aprendizaje Supervisado, es necesario que todos los datos estén _etiquetados_ con su correspondiente _respuesta correcta_ o _esperada_ la cual se utilizará para evaluar el rendimiento del modelo. El tipo de datos que se puede utilizar es altamente variado, desde imágenes, texto plano, vídeos hasta simples valores numéricos; la limitante es que puedan se representados de forma numérica para ingresarlos a las capas de entrada.\n",
    "\n",
    "#### Arquitectura\n",
    "Es necesario definir qué estructura (estática) se desea utilizar (i.e. CNN, RNN, ANN, etc.), así como la estructura (número de capas ocultas, neuronas en cada una, funciones de activación, etc.). Recuerda que hay diversas redes _ideales_ para cierto tipo de datos de entrada, pero no te limites, **deja salir tu creatividad**.\n",
    "\n",
    "#### Función de pérdida\n",
    "Es la forma de evaluar si el modelo está haciendo las cosas bien o no, nuevamente dependerá de tu caso, pero las más comúnes son:\n",
    "\n",
    "**Regresión**\n",
    "- Mean Absolute Error (error absoluto promedio) $MAE = \\frac{1}{n}\\sum_{i=1}^{n}|{Y_i - \\hat{Y}_i}|$\n",
    "- Mean Squared Error (error cuadrático medio) $MSE = \\frac{1}{n}\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2$\n",
    "\n",
    "**Clasificación**\n",
    "- Binary Cross Entropy (clasificación binaria) $H_p(q) = -\\frac{1}{n}\\sum_{i=1}^{n}Y_i \\dot log(p(Y_i)) + (1 - Y_i) \\dot log(1 - p(Y_i))$ donde $p(Y_i) = \\hat{Y}_i$, puedes indagar un poco más [aquí](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)\n",
    "- Categorical Cross Entropy (clasificación de múltiples categorías)\n",
    "\n",
    "#### Optimizador\n",
    "Para poder actualizar los parámetros de cada capa es necsario definir un algoritmo para hacerlo, ese es el optimizador, es decir, qué regla (fórmula matemática) vamos a seguir para actualizarlo. Esto está relacionado con el proceso de _backpropagation_ que es la clave para actualizar los valores dependiendo de la pérdida (error) que hubo entre $Y_i$ y $\\hat{Y}_i$ (también denotada como $Y^*$ (ver Fig. 7) ¨**pero no te espantes, no debes de comprender todas las operaciones matemáticas, tan solo conocer cómo es el proceso**.\n",
    "\n",
    "> <img src=\"../figs/backpropagation.png\" width=600 />\n",
    ">\n",
    "> Fig. 7: Diagrama y algoritmo de backpropagation, así como regla de actualización de parámetros (optmimización) como se ilustra en la sección (b) en el punto (ii).\n",
    ">\n",
    "> Fuente: [Backpropagation, Medium](https://medium.com/@jorgesleonel/backpropagation-cc81e9c772fd)\n",
    "\n",
    "Algunos de los algoritmos más usados son:\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "- ADAM\n",
    "\n",
    "Puedes encontrar información sobre estos y muchos otros en [este artículo en Medium](https://medium.com/mlearning-ai/optimizers-in-deep-learning-7bf81fed78a0).\n",
    "\n",
    "Una vez tienes estos cuatro pilares básicos, **¡estamos list@s para entrenar nuestro modelo de DL!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f54856-1803-4408-8f31-f84647fd6902",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### DL 101: \"Hello Neural Networks\"\n",
    "Para crear modelos de DL en Python una de las bibliotecas más utilizadas es [TensorFlow](https://www.tensorflow.org/about), desarrollada por Google y de código abierto (open source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d9fe8-19f1-4807-9b47-c0c28d2d4a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Versión de TensorFlow:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a44853-066f-4873-8292-d5efd5e93c88",
   "metadata": {},
   "source": [
    "#### Datos\n",
    "\n",
    "Ocuparemos una base de datos muy utilizada en DL y que es abierta para su uso, conocida como MNIST ([sitio oficial](http://yann.lecun.com/exdb/mnist/), [artículo en Wikipedia](https://en.wikipedia.org/wiki/MNIST_database)) esta base de datos consiste en imágenes con dígitos del 0 al 9 escritos a mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c089e3-c07a-4c79-85a1-51cf4a0520c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist ## Cargamos localmente el dataset que está precargado en tensorflow\n",
    "\n",
    "## Lo leemos y a su vez separamos en set de train y test\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb16f1-f876-4b99-847c-bda5f0d50a96",
   "metadata": {},
   "source": [
    "Exploremos ahora un poco el dataset para saber con qué estamos trabajando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c950af9-bc41-47a5-80a2-7ef01d6ab68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape) # De aquí podemos ver cuántas imágenes hay en el set de train, y qué tamaño tienen\n",
    "print(x_test.shape) # Igualmente para el de test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33803b5c-3e4b-45cb-87c8-59e4ededc601",
   "metadata": {},
   "source": [
    "Es decir, hay 60,000 imágenes en el set de train, y 10,000 en el de test. Todas estas imágenes son de 28x28 pixeles. Vamos a explorarlas un poco más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a5773-d220-40d3-852e-9d90379f8b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunas bibliotecas que nos van a ser de mucha ayuda\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Veamos cómo luce una imagen\n",
    "imagen = x_train[0]\n",
    "print(imagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b9ff5-9b42-4aeb-8b3e-0d84e86a6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejor veamos cómo serían esos números convertidos a una imagen\n",
    "plt.figure()\n",
    "plt.imshow(imagen, cmap=plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a18fa7-5fbb-4a3f-965b-8044a06a05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cómo luce la \"predicción esperada\" que corresponde a esa imagen\n",
    "prediccion = y_train[0]\n",
    "print(prediccion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6062eb11-e4fe-4fc9-b562-eafd2c62e9a0",
   "metadata": {},
   "source": [
    "Como pudiste notar tanto en la matriz, como en la escala de colores de la imagen, los valores van desde 0 hasta 255 (código RGB de 8-bits usualmente), una técnica muy común cuando se trabaja con imágenes es transformarlas y pasar de un rango [0, 255] a uno [0, 1] de forma que los valores no tiendan a infinito si se mutiplican en algún momento.\n",
    "\n",
    "Podemos hacer eso fácilmente porque nuestras imágenes son tensores (algo muy parecido a un arreglo de Numpy multi-dimensional) a los que les podemos aplicar operaciones matemáticas de la forma que ya conocemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b9fca-7ac3-491e-809d-663c154da37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255, x_test / 255 # dividimos entre el valor máximo para normalizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb028f8-4840-491c-bee8-22ffb39a0207",
   "metadata": {},
   "source": [
    "Veamos cómo lucen algunos de nuestras imágenes en el set de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ea635-92fa-4874-ac8b-11a24ef60990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardemos cuántas imágenes hay en el set de train\n",
    "train_len = x_train.shape[0]\n",
    "\n",
    "# Hagamos múltiples sub-gráficas en una misma\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    indice_random = np.random.randint(train_len) # seleccionamos un índice de imagen al azar\n",
    "    imagen_random = x_train[indice_random]\n",
    "    plt.imshow(imagen_random, cmap=plt.cm.binary) # desplegamos la imagen\n",
    "    plt.xlabel(y_train[indice_random]) # desplegamos su correspondiente predicción esperada\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb33232a-dacd-46ae-a992-7137711e00ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Arquitectura\n",
    "Veamos la parte más intrigante de construir un modelo de DL, la arquitectura de la red.\n",
    "\n",
    "En esta ocasión vamos a entrenar un modelo tipo ANN simple, con únicamente una capa oculta, para eso vamos a ocupar la función [`Sequential` de TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential), la cual nos permite ir agregando capas de forma secuencial como si fueran una lista de pasos.\n",
    "\n",
    "En este caso nuestra red va a tener una forma del tipo:\n",
    "> - Capa de entrada de **28x28 (784) neuronas**\n",
    "> - Una **capa oculta con 300 neuronas y función de activación del tipo ReLU** ([más aquí](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6))\n",
    ">> <img src=\"../figs/activation_functions.png\" width=400 />\n",
    ">> \n",
    ">> Fig. 8: Gráficas de diferentes funciones de activación comúnmente utilizadas\n",
    "> - Una estrategia del tipo **_dropout_ con probabilidad 0.5**\n",
    ">> <img src=\"../figs/dropout.png\" width=400 />\n",
    ">>\n",
    ">> Fig. 9: Esquema de la funcionalidad de una estrategia de _dropout_ donde antes de la iteración todos los pesos $w_i \\neq 0$, y en la iteración siguiente un cierto número de conexiones _j_ (dado por la probabilidad) tiene peso $w_j = 0$ durante esa iteración.\n",
    "> - Una capa de salida con **10 neuronas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df7951-de0b-4103-9083-20d4bda866b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestro primer modelo en tensorflow\n",
    "tamano_imagen = imagen.shape # tamaño de las imágenes en pixeles\n",
    "numero_categorias = len(np.unique(y_train)) # número de categorías a predecir\n",
    "\n",
    "primer_modelo = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=tamano_imagen),\n",
    "  tf.keras.layers.Dense(300, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Dense(numero_categorias)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698057b7-9d6c-4483-b07e-661366cf708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retomamos nuestra imagen del 5 que ya habíamos ocupado antes\n",
    "plt.imshow(x_train[:1][0], cmap=plt.cm.binary);\n",
    "print(x_train[:1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d487d59-4d9f-476e-9489-cf4e1a4c5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos qué sucede si aplicamos nuestro modelo a una de nuestras imágenes del set de train\n",
    "prediccion_primer_modelo = primer_modelo(x_train[:1]).numpy()\n",
    "print(prediccion_primer_modelo.shape)\n",
    "prediccion_primer_modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b5c482-c8fd-42c1-a448-c081e89cc148",
   "metadata": {},
   "source": [
    "¿Qué significa este resultado? Bueno, simplemente son resultados que arroja la capa de salida de nuestro modelo, que si recordamos tenía 10 neuronas, mismas que nos están dando un valor final y por tanto un vector de dimensión 10. Y eso quiere decir que... ¿Cuál es la predicción entonces?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55ff68-cf9b-42ae-81ca-9aa65688fe0f",
   "metadata": {},
   "source": [
    "##### Regresión vs Clasificación\n",
    "\n",
    "Si nuestro problema fuera de regresión, seguramente este valor nos sería de mucha utilidad y podríamos tomarlo como la predicción del modelo. Sin embargo, nuestro problema es de clasificación **¿cierto?**.\n",
    "\n",
    "Para situaciones de clasificación usualmente se utiliza una capa extra, conocida como _softmax_ y que nos permite crear una distribución donde la suma de todos los valores del vector de salida sea 1. En conclusión, algo que podemos interpretar como una distribución de probabilidades, y por tanto cada valor del vector representará la probabilidad de que la respuesta sea cada dígito de las opciones posibles (en este caso diez, dpigitos del 0 al 9). Veamos esto en acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e8e1b5-bfb8-4dbb-9c0a-7851ae288860",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_probabilidades = tf.nn.softmax(prediccion_primer_modelo).numpy()\n",
    "print(tensor_probabilidades.shape)\n",
    "probabilidades = tensor_probabilidades[0] # En realidad es un tensor con un único elemento, que es la predicción a nuestra primera imagen\n",
    "probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653fe80e-c3cc-4de5-87df-30e356557195",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, proba in enumerate(probabilidades):\n",
    "    print(f\"La probabilidad de que sea un {i} es {proba:.3f}\")\n",
    "\n",
    "print(f\"\\nLa probabilidad más alta es que sea un dígito {np.argmax(probabilidades)} con probabilidad {np.max(probabilidades):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9b6a6-7359-4594-94b0-23289822055b",
   "metadata": {},
   "source": [
    "#### Función de pérdida \n",
    "Como podemos ver, el modelo en realidad es bastante malo para predecir y en este caso es evidente para nosotros que se equivocó, pero para comunicarle eso es necesario utilizar una función de pérdida, la cual le diga de forma matemática al modelo qué tanto y cómo se equivocó.\n",
    "\n",
    "Para este ejemplo vamos a utilizar la función _Sparse Categorical Cross Entropy_ que nos permite evaluar entre varias clases y no solo de forma binaria (puedes ver más [de esta función de pérdida](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy), o la lista de [todas las disponibles en TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses)) y también considera que tú puedes hacer tus propias funciones de pérdida si es necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d4538-57eb-4770-bdf0-1dbb5cead504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos una nueva instancia de esta función de pérdida, la cual vamos a ocupar para medir el error\n",
    "funcion_perdida = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # from_logits hace referencia a que tiene que convertir primero a distribución de probabilidades\n",
    "funcion_perdida(y_train[:1], prediccion_primer_modelo).numpy() # Una respuesta aleatoria sería equivalente a tf.math.log(1/10) ~= 2.3 (1/10 de probabilidad de que fuera correcta por simple azar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60be848-4b06-4459-a82c-a4b8e5ba09c9",
   "metadata": {},
   "source": [
    "Es decir, la probabilidad de que salga la categoría correcta es equivalente a que saliera por simple azar dado que $log(\\frac{1}{10}) \\approx 2.3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae7e35c-0df1-4cb4-a011-5d55742ca8f9",
   "metadata": {},
   "source": [
    "#### Optimizador\n",
    "Ya que tenemos cómo medir el error, ahora solo falta definir la forma matemática en que vamos a actualizar los parémetros durante el entrenamiento, en otras palabras un optimizador. En este caso vamos a partir del más común, el Stochastic Gradient Descente (SGD), pero no dudes en revisar todos los disponibles en [la lista de TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b96a52-524e-4bf3-9629-d14cdcfb0a8e",
   "metadata": {},
   "source": [
    "#### Compilar (TensorFlow)\n",
    "Cuando creamos un modelo de deep learning en TensorFlow es conveniente compilarlo antes de empezar con el entrenamiento, esto nos permite que sea más rápido y controlado, así como par definir las características que va a tener el entrenamiento, como lo es la función de pérdida y el optimizador a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be600d5-7daa-42dc-a708-6ba85af1343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "primer_modelo.compile(optimizer='sgd', loss=funcion_perdida, metrics=['accuracy']) # compilamos nuestro modelo y definimos que la forma de medir su rendimiento sea a través del 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7173ad9a-beb4-4e84-859a-761d844faa82",
   "metadata": {},
   "source": [
    "#### Entrenamiento\n",
    "Una vez que ya tenemos nuestro modelo compilado, el último paso es comenzar a entrenar con los datos. ¡Sí, por fin lo lograste! En este caso vamos a darle el set de train para entrenar, y definimos que haga **5** vueltas de entrenamiento con el set completo (_epochs_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c8e5e-bca9-43d5-80b9-4131ca393358",
   "metadata": {},
   "outputs": [],
   "source": [
    "primer_modelo.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0904a70c-741a-4ee7-a602-3c719eed9cf1",
   "metadata": {},
   "source": [
    "#### Evaluación\n",
    "**¡Así de fácil!** Qué te parece si ahora lo evaluamos sobre el set de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35620d-c74c-40d0-9da7-8ff1d99ced22",
   "metadata": {},
   "outputs": [],
   "source": [
    "primer_modelo.evaluate(x_test,  y_test, verbose=2) # Obtenemos las métricas sobre el set de test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126816e9-df8d-4f60-b5a9-0ddf731ca175",
   "metadata": {},
   "source": [
    "Veamos esto un poco más claro retomando nuestro dígito 5 de más arriba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc764877-c53d-4388-a3cb-0727c8708ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retomamos nuestra imagen del 5 que ya habíamos ocupado antes\n",
    "plt.imshow(x_train[:1][0], cmap=plt.cm.binary);\n",
    "print(x_train[:1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd86f60-aebb-4b5a-91f2-7a525ed90ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos la predicción del modelo entrenado\n",
    "pred = primer_modelo(x_train[:1]).numpy()\n",
    "probabilidades = tf.nn.softmax(pred).numpy()\n",
    "print(probabilidades)\n",
    "print(f\"\\nLa probabilidad más alta es que sea un dígito {np.argmax(probabilidades[0])} con probabilidad {np.max(probabilidades[0]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc8720-a513-4696-a86a-036b67e589dd",
   "metadata": {},
   "source": [
    "---\n",
    "### Ejercicio 1\n",
    "Ahora que tenemos un modelo básico funcional, ¡es momento de explorar! Prueba modificando tu modelo (arquitectura, función de pérdida, optimizador, épocas de entrenamiento) para ver si puedes conseguir un mejor rendimiento. ¡Éxito!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba35116b-00a4-49cc-8ff9-1735b3948cb7",
   "metadata": {},
   "source": [
    "---\n",
    "#### Mejoras menores\n",
    "Podemos evitarnos la necesidad de evaluar dentro de la función de pérdida las probabilidades, y meterlas en nuestro modelo, de forma que la salida sea en sí la distribución de probabilidad de las posibles categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e8332-53fd-4860-b357-08658a056fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos hacerlo agregrando una nueva capa al modelo que ya teníamos, o volviendo a construirlo y agregándola al final de la secuencia de capas\n",
    "modelo_probabilidad = tf.keras.Sequential([\n",
    "  primer_modelo,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "# Que es equivalente a este\n",
    "modelo_probabilidad = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=tamano_imagen),\n",
    "  tf.keras.layers.Dense(300, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Dense(numero_categorias),\n",
    "  tf.keras.layers.Softmax() # nueva capa\n",
    "])\n",
    "\n",
    "pred_modelo_proba = modelo_probabilidad(x_train[:1]).numpy()\n",
    "# Vemos cómo luce ahora la salida en comparación con la anterior\n",
    "print(f\"Salida primer modelo: {prediccion_primer_modelo}\\nSalida modelo probabilidad: {pred_modelo_proba}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f13d2-1ef0-407d-928b-5c6e2b7e2c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos hacer el proceso de entrenar nuestro modelo de probbailidad, pero hay que hacer cambios mínimos, principalmente en la función de pérdida\n",
    "funcion_perdida_proba = tf.keras.losses.SparseCategoricalCrossentropy() # ahora que la salida es una densidad de probabilidades, podemos ignorar el parámetro `from_logits` ya que el _default_ es `False`\n",
    "modelo_probabilidad.compile(optimizer='sgd', loss=funcion_perdida_proba, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab903b62-65c0-435e-aa53-eaeb8bb848d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_probabilidad.fit(x_train, y_train, epochs=5) # Entrenamos el nuevo modelo\n",
    "modelo_probabilidad.evaluate(x_test,  y_test, verbose=2) # Obtenemos las métricas sobre el set de test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c6ec36-28fe-47fd-b959-4af9f070d4de",
   "metadata": {},
   "source": [
    "¡Impresionante, ahora sí ya hiciste tu primer gran entrada al mundo de Deep Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0021ac99-7d9c-4837-b086-7b095033e8ed",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluación con datos reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b71257-d14b-4c58-927a-13211a0e8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos la prueba con sus imágenes, para probar la red en un caso real\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "path = os.path.join('..', 'ENENIST')\n",
    "for im_path in os.listdir(path):\n",
    "    dot_split = im_path.split('.')\n",
    "    if dot_split[-1] in [\"png\", \"jpg\", \"jpeg\"]:\n",
    "        try:\n",
    "            label = int(dot_split[0].split('_')[-1])\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e} with image '{im_path}'\")\n",
    "            continue\n",
    "        im = Image.open(os.path.join(path, im_path))\n",
    "        im = im.resize((28, 28))\n",
    "        im = ImageOps.grayscale(im)\n",
    "        im_arr = np.asarray(im)\n",
    "        images.append(im_arr)\n",
    "        labels.append(label)\n",
    "\n",
    "x_new = tf.convert_to_tensor(images)\n",
    "#y_new = tf.convert_to_tensor(labels)\n",
    "y_new = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588cbce-daa9-4c66-b828-03f7e9fdc33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = modelo_probabilidad(x_new).numpy()\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(len(images)):\n",
    "    plt.subplot(len(images)//5+1,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_new[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(f\"Real: {y_new[i]}   Pred: {np.argmax(preds[i])}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3da587-f630-40a3-96b0-91d9aa23f081",
   "metadata": {},
   "source": [
    "---\n",
    "### Fashion MNIST\n",
    "Probemos entrenar un modelo con otro dataset, ahora llamado _Fashoin MNIST_, el cual es muy similar a _MNIST_, pero de ropa. Nuevamente hay 10 categorías, en las cuales se pueden clasificar las imágenes que lo contienen, son las siguientes:\n",
    "\n",
    "|Etiqueta (label)|Categoría|\n",
    "|-|-|\n",
    "|0|T-shirt/top|\n",
    "|1|Trouser|\n",
    "|2|Pullover|\n",
    "|3|Dress|\n",
    "|4|Coat|\n",
    "|5|Sandal|\n",
    "|6|Shirt|\n",
    "|7|Sneaker|\n",
    "|8|Bag|\n",
    "|9|Ankle boot|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355064c0-1c5f-4d63-ba9b-ae4f1a1bbfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "print(f\"Imágenes en set de train: {x_train.shape[0]} de dimensión: {x_train.shape[1:]}\\nImágenes en set de test: {x_test.shape[0]}\")\n",
    "# Veamos cómo lucen las etiquetas\n",
    "print(f\"Etiquetas: {y_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367df580-b46f-40a9-a5b9-d7325ffccfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para comprender mejor de forma humana, podemos hacer un arreglo con las categorías como texto\n",
    "nombres_clases = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] # lista de categorías en su índice correspondiente\n",
    "\n",
    "# Veamos cómo lucen las etiquetas convertidas a texto\n",
    "mapeo_etiqueta = lambda x: (list(map(lambda i: nombres_clases[i], x))) if type(x) != type(np.uint8()) else nombres_clases[x]\n",
    "etiquetas = mapeo_etiqueta(y_train)\n",
    "etiquetas[:10] # solo veamos las primeras 10 por simplicidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffe3c6-0bfe-4bd8-a702-c1535d949b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos una de las imágenes\n",
    "plt.figure()\n",
    "plt.imshow(x_train[0], cmap=plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a845a4ef-b598-49ed-872d-448b4dff619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizamos las imágenes\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db724e29-4bde-4e66-a60a-50adfe13c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analicemos nuevamente un poco más de imágenes\n",
    "train_len = x_train.shape[0]\n",
    "\n",
    "# Hagamos múltiples sub-gráficas en una misma\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    indice_random = np.random.randint(train_len) # seleccionamos un índice de imagen al azar\n",
    "    imagen_random = x_train[indice_random]\n",
    "    plt.imshow(imagen_random, cmap=plt.cm.binary) # desplegamos la imagen\n",
    "    y = y_train[indice_random]\n",
    "    plt.xlabel(f\"{mapeo_etiqueta(y)} ({y})\") # desplegamos su correspondiente predicción esperada, según el nombre de la clase correspondientes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16225478-2e44-421a-81c4-b6087914d03a",
   "metadata": {},
   "source": [
    "---\n",
    "### Ejercicio 2\n",
    "Muy bien, ya tenemos los datos listos, ¡ahora hay que terminar los pasos restantes!\n",
    "Deberás:\n",
    "- Crear una arquitectura de red para entrenar con los nuevos datos\n",
    "- Seleccionar una función de pérdida que te permita evaluar de forma adecuada el error durante el entrenamiento\n",
    "- Elegir un optimizador para actualizar los parámetros de la red\n",
    "- Indicar el número de épocas que deseas que se entrene el modelo\n",
    "\n",
    "Una vez que tengas eso, ¡evalúa su desempeño en el set de test!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35edea3b-1fde-4b4c-adc6-4f066019ceb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "###### Ayudas para graficar\n",
    "Algunas funciones útiles para imprimir de forma más interesante los resultados del modelo (no te preocupes si no comprendes todo lo que está sucediendo en cada línea del código, solo son funciones adicionales complementarias para un mejor despliegue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b8aeeb-a128-40ce-abb3-de82f19eb895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Imprimimos una imagen en escala de grises, e indicamos la categoría predicha y la probabilidad con que lo asegura el modelo\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    true_label, img = true_label[i], img[i] # obtenemos los datos \n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img, cmap=plt.cm.binary) # desplegamos la imagen\n",
    "    \n",
    "    # Obtenemos la predicción de mayor probabilidad, y la pintamos según si fue correcta (azul) o incorrecta (roja)\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    # Añadimos esa leyenda a la figura\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "\n",
    "# Gráfica de barras con la distribución de probabilidad para todas las categorías\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    true_label = true_label[i] # obtenemos la predicción correcta\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10)) # colocamos 10 barras (número de categorías posibles)\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\") # hacemos una gráfica de barras\n",
    "    plt.ylim([0, 1]) # definimos el rango máximo\n",
    "    predicted_label = np.argmax(predictions_array) # obtenemos la predicción con mayor probabilidad\n",
    "\n",
    "    thisplot[predicted_label].set_color('red') # pintamos todas las erróneas de color rojo\n",
    "    thisplot[true_label].set_color('blue') # pintamos la correcta de azul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6768e259-de9a-4bdd-8024-04f79291869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=3)\n",
    "\n",
    "modelo_con_probabilidad = tf.keras.Sequential([model, \n",
    "                                         tf.keras.layers.Softmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b432d5d-a181-453b-b695-7f12bde14218",
   "metadata": {},
   "source": [
    "#### Visualizamos las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a022194-fc02-4410-a66c-7f82f3adf5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ¡OJO: aquí debes de poner el nombre que le hayas asignado a tu modelo! Y presta atención a que es necesario que la predicción ya sea una distribución de probabilidades\n",
    "predicciones = modelo_con_probabilidad.predict(x_test)\n",
    "\n",
    "i = 0 # índice de la imagen a visualizar\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(i, predicciones[i], y_train, x_train)\n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(i, predicciones[i],  y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934708d-51a3-479f-b6c2-e3ac9345ecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imprimimos algunos más para analizar un comportamiento general\n",
    "n_filas = 5\n",
    "n_columnas = 3\n",
    "n_imagenes = n_filas*n_columnas\n",
    "plt.figure(figsize=(2*2*n_columnas, 2*n_filas))\n",
    "for i in range(n_imagenes):\n",
    "    plt.subplot(n_filas, 2*n_columnas, 2*i+1)\n",
    "    plot_image(i, predicciones[i], y_test, x_test)\n",
    "    plt.subplot(n_filas, 2*n_columnas, 2*i+2)\n",
    "    plot_value_array(i, predicciones[i], y_test)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a5d15-34d5-4cf7-96ea-cc52d3fa2b13",
   "metadata": {},
   "source": [
    "---\n",
    "#### Ejercicio 3 (opcional)\n",
    "¿Crees poder lograr un mejor rendimiento? Prueba modificar tu aruitectura, optimizador, etc. ¡pero ten cuidado con el overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e82771-2a9b-474f-af58-1e7597b2c1f5",
   "metadata": {},
   "source": [
    "---\n",
    "Fuentes:\n",
    "1. [Tutorial de TensorFlow sobre MNIST](https://www.tensorflow.org/tutorials/quickstart/beginner)\n",
    "2. [Tutorial de TensorFlow sobre Fashion MNIST](https://www.tensorflow.org/tutorials/keras/classification)\n",
    "---\n",
    "### Reto\n",
    "Si quieres seguir explorando cómo crear modelos de DL con TensorFlow cada vez más complejos y versátiles, prueba con el [siguiente tutorial de TensorFlow](https://www.tensorflow.org/tutorials/keras/text_classification_with_hub) para clasificación a partir de lenguaje natural (texto).\n",
    "Y si te sientes cómodo, no dudes en visitar el sitio web de [Kaggle](https://www.kaggle.com/) es un recurso básico e impresionante para el mundo del DL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
